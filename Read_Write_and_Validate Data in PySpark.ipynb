{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read_Write_and_Validate Data in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to always begin every Spark Session by creating a Spark instance. We can do so as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ReadWriteVal</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f50522ea48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadWriteVal\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "\n",
    "# This will help us to get a link to the Spark UI \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a CSV file in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/kava2/Documents/Udemy/Spark_Datasets/'\n",
    "\n",
    "pga = spark.read.csv(path + 'pga_tour_historical.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) View First 5 lines of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us view the first five lines of the dataframe to see what is inside it. We can do this two ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------------+--------------------+-----+\n",
      "|    Player Name|Season|       Statistic|            Variable|Value|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "|Robert Garrigus|  2010|Driving Distance|Driving Distance ...|   71|\n",
      "|   Bubba Watson|  2010|Driving Distance|Driving Distance ...|   77|\n",
      "| Dustin Johnson|  2010|Driving Distance|Driving Distance ...|   83|\n",
      "|Brett Wetterich|  2010|Driving Distance|Driving Distance ...|   54|\n",
      "|    J.B. Holmes|  2010|Driving Distance|Driving Distance ...|  100|\n",
      "+---------------+------+----------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pga.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Name</th>\n",
       "      <th>Season</th>\n",
       "      <th>Statistic</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Robert Garrigus</td>\n",
       "      <td>2010</td>\n",
       "      <td>Driving Distance</td>\n",
       "      <td>Driving Distance - (ROUNDS)</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Bubba Watson</td>\n",
       "      <td>2010</td>\n",
       "      <td>Driving Distance</td>\n",
       "      <td>Driving Distance - (ROUNDS)</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Dustin Johnson</td>\n",
       "      <td>2010</td>\n",
       "      <td>Driving Distance</td>\n",
       "      <td>Driving Distance - (ROUNDS)</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Brett Wetterich</td>\n",
       "      <td>2010</td>\n",
       "      <td>Driving Distance</td>\n",
       "      <td>Driving Distance - (ROUNDS)</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>J.B. Holmes</td>\n",
       "      <td>2010</td>\n",
       "      <td>Driving Distance</td>\n",
       "      <td>Driving Distance - (ROUNDS)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Player Name  Season         Statistic                     Variable  \\\n",
       "0  Robert Garrigus    2010  Driving Distance  Driving Distance - (ROUNDS)   \n",
       "1     Bubba Watson    2010  Driving Distance  Driving Distance - (ROUNDS)   \n",
       "2   Dustin Johnson    2010  Driving Distance  Driving Distance - (ROUNDS)   \n",
       "3  Brett Wetterich    2010  Driving Distance  Driving Distance - (ROUNDS)   \n",
       "4      J.B. Holmes    2010  Driving Distance  Driving Distance - (ROUNDS)   \n",
       "\n",
       "  Value  \n",
       "0    71  \n",
       "1    77  \n",
       "2    83  \n",
       "3    54  \n",
       "4   100  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pga.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that converting the Spark dataframe to a pandas dataframe using the toPandas() method is not time or memory efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Print the Schema Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us print the details of the dataframes schema that Spark infered to ensure that it was infered correctly. Sometimes it is not infered correctly, so we need to watch out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player Name: string (nullable = true)\n",
      " |-- Season: integer (nullable = true)\n",
      " |-- Statistic: string (nullable = true)\n",
      " |-- Variable: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      "\n",
      "None\n",
      " \n",
      "['Player Name', 'Season', 'Statistic', 'Variable', 'Value']\n",
      " \n",
      "DataFrame[summary: string, Player Name: string, Season: string, Statistic: string, Variable: string, Value: string]\n"
     ]
    }
   ],
   "source": [
    "print(pga.printSchema())\n",
    "print(\" \")\n",
    "print(pga.columns)\n",
    "print(\" \")\n",
    "\n",
    "# We can also use the describe() method \n",
    "print(pga.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Edit the Schema during the Read-In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the output above that Spark did not correctly infer that the \"value\" column was an integer value. Let's try specifying the schema this time to let spark know what the schema should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"Player Name\", StringType(), True), \\\n",
    "               StructField(\"Season\", IntegerType(), True), \\\n",
    "               StructField(\"Statistic\", StringType(), True), \\\n",
    "               StructField(\"Variable\", StringType(), True), \\\n",
    "               StructField(\"Value\", IntegerType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struct = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can specify our custom schema while reading the data\n",
    "path = 'C:/Users/kava2/Documents/Udemy/Spark_Datasets/'\n",
    "\n",
    "pga = spark.read.csv(path + 'pga_tour_historical.csv', schema = final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Player Name: string (nullable = true)\n",
      " |-- Season: integer (nullable = true)\n",
      " |-- Statistic: string (nullable = true)\n",
      " |-- Variable: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pga.printSchema())\n",
    "# That's better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Generate Summary Statistics for only one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate summary statistics for only the \"Value\" column using the .describe function\n",
    "\n",
    "(count, mean, stddev, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             Value|\n",
      "+-------+------------------+\n",
      "|  count|           1657247|\n",
      "|   mean|12494.388998743096|\n",
      "| stddev|157274.75673570728|\n",
      "|    min|              -178|\n",
      "|    max|           3564954|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pga.describe(['Value']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Generate Summary Statistics for Two Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to generate ONLY the count min and max for BOTH the \"Value\" and \"Season\" variable using the select. You can't use the .describe function for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|summary|  Value| Season|\n",
      "+-------+-------+-------+\n",
      "|  count|1657247|2740403|\n",
      "|    min|   -178|   2010|\n",
      "|    max|3564954|   2018|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pga.select('Value', 'Season').summary('count', 'min', 'max').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Writing a Parquet File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try writing a parquet file (not partitioned) from the pga dataset. \n",
    "\n",
    "But first let us create a new dataframe containing ONLY the the \"Season\" and \"Value\" fields (using the \"select command ) and then write a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pga.select('Season', 'Value')\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet('partition_parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if any of our variable names contain spaces, spark will produce an error message with this call. That is why we are selecting ONLY the \"Season\" and \"Value\" fields. Ideally we should have renamed those columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a partitioned parquet file (partitioned by the column 'Season')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Write a Partitioned Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Season|Value|\n",
      "+------+-----+\n",
      "|  null| null|\n",
      "|  2010|   71|\n",
      "|  2010|   77|\n",
      "|  2010|   83|\n",
      "|  2010|   54|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"Season\").parquet(\"partitioned_parquet/\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Read-In a Partitioned Parquet File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall read the parquet file that we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Value|Season|\n",
      "+-----+------+\n",
      "|   71|  2010|\n",
      "|   77|  2010|\n",
      "|   83|  2010|\n",
      "|   54|  2010|\n",
      "|  100|  2010|\n",
      "|   63|  2010|\n",
      "|   88|  2010|\n",
      "|   64|  2010|\n",
      "|   64|  2010|\n",
      "|   92|  2010|\n",
      "|   75|  2010|\n",
      "|   54|  2010|\n",
      "|   76|  2010|\n",
      "|   94|  2010|\n",
      "|   82|  2010|\n",
      "|   85|  2010|\n",
      "|   79|  2010|\n",
      "|   89|  2010|\n",
      "|   88|  2010|\n",
      "|   91|  2010|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"partitioned_parquet/\" #Note: if you add a * to the end of the path, the Season var will be automatically dropped\n",
    "parquet = spark.read.parquet(path)\n",
    "        \n",
    "parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Reading-In a Set of Partitioned Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall only read Seasons 2010, 2011 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Value|\n",
      "+-----+\n",
      "|   71|\n",
      "|   77|\n",
      "|   83|\n",
      "|   54|\n",
      "|  100|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that this method only gives you the \"Value\" column\n",
    "path = \"partitioned_parquet/\"\n",
    "partitioned = spark.read.parquet(path+'Season=2010/',\\\n",
    "                             path+'Season=2011/', \\\n",
    "                             path+'Season=2012/')\n",
    "\n",
    "partitioned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Value|Season|\n",
      "+-----+------+\n",
      "|   71|  2010|\n",
      "|   77|  2010|\n",
      "|   83|  2010|\n",
      "|   54|  2010|\n",
      "|  100|  2010|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to use this method to get the \"Season\" and \"Value\" Columns\n",
    "path = \"partitioned_parquet/\"\n",
    "dataframe = spark.read.option(\"basePath\", path).parquet(path+'Season=2010/',\\\n",
    "                                                                path+'Season=2011/', \\\n",
    "                                                                path+'Season=2012/')\n",
    "dataframe.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Create your Own DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the spark.createDataFrame function to create our own data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+------+\n",
      "|     name|age| AB|Number|\n",
      "+---------+---+---+------+\n",
      "|     Kyle| 10|  A|     1|\n",
      "|Melbourne| 36|  A|     1|\n",
      "|     Nina|123|  A|     1|\n",
      "|  Stephen| 48|  B|     2|\n",
      "|   Orphan| 16|  B|     2|\n",
      "|    Imran|  1|  B|     2|\n",
      "+---------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = [('Kyle',10,'A',1),('Melbourne',36,'A',1),('Nina',123,'A',1),('Stephen',48,'B',2),('Orphan',16,'B',2),('Imran',1,'B',2)]\n",
    "df = spark.createDataFrame(values,['name','age','AB','Number'])\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
